local stringx = require('pl.stringx')
local cmd = torch.CmdLine()
cmd:option("-beam_size",7,"beam_size")
cmd:option("-batch_size",128,"decoding batch_size")
cmd:option("-params_file","../../Atten/save_t_given_s/params","")
cmd:option("-model_file","../../Atten/save_t_given_s/model1","")
cmd:option("-setting","BS","setting for decoding, sampling, BS, DiverseBS,StochasticGreedy")
cmd:option("-DiverseRate",0,"")
cmd:option("-InputFile","","")
cmd:option("-OutputFile","","")
cmd:option("-max_length",0,"")
cmd:option("-min_length",0,"")
cmd:option("-NBest",true,"output N-best list or just a simple output")
cmd:option("-gpu_index",1,"the index of GPU to use")
cmd:option("-allowUNK",true,"the index of GPU to use")
cmd:option("-MMI",false,"")
cmd:option("-onlyPred",true,"")
cmd:option("-MMI_params_file","","")
cmd:option("-MMI_model_file","","")
cmd:option("-max_decoded_num",0,"")
cmd:option("-output_source_target_side_by_side",false,"")
cmd:option("-StochasticGreedyNum",1,"")
cmd:option("-target_length",0,"force the length of the generated target, 0 means there is no such constraints")
cmd:option("-FuturePredictorModelFile","","")
cmd:option("-PredictorWeight",0,"length of target sequences you wish to generate")
cmd:option("-dictPath","../../data/movie_25000","")
cmd:option("-Task","length","length,backward")

local params= cmd:parse(arg)
print(params)
return params;
